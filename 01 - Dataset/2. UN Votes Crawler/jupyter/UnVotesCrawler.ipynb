{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c799d93-229e-466b-b82f-65130ddee655",
   "metadata": {},
   "source": [
    "# TFB IA & BigData XII _ KeepCoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fc44d-685b-4472-8aa9-cd576fea3527",
   "metadata": {},
   "source": [
    "## CRAWLER UNVOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad8ec8-1da4-4a12-b4fb-591d9fc24120",
   "metadata": {},
   "source": [
    "A continuación presentamos nuestro trabajo en este área de recopilación del dato. Tras determinar la fuente de datos con la trajaremos en nuestro proyecto, se ha determinado la siguiente url como objetivo de scraping:\n",
    "\n",
    "https://digitallibrary.un.org/search?ln=en&cc=Voting+Data&p=&f=&action_search=Search&rm=&sf=&so=d&rg=50&c=Voting+Data&c=&of=hb&fti=0&fti=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca96b9-b229-4e4c-9f8e-a6c38c84d757",
   "metadata": {},
   "source": [
    "### Una breve explicación sobre este proceso de extracción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f46e7-4926-4433-99d2-cee498376829",
   "metadata": {},
   "source": [
    "En primer lugar, usaremos una biblioteca de python llamada selenium debido al prestigio en su labor de interactuar con las páginas web (haciendo click, por ejemplo) gracias a su webdriver (navegador controlado por el script), en este caso usaremos firefox.\n",
    "\n",
    "Importamos la libreria anteriormente descrita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f91de-1c62-463b-9ff3-4b8b2b72a290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7d6b5-a031-46fa-81e1-9b319edeb035",
   "metadata": {},
   "source": [
    "El siguiente código tiene como objetivo extraer todas las url contenidas en los hrefs de nuestro interés para posteriormente almacenarlas en un array y escribirlas en disco (archivo ./href) para su ulterior procesamiento individual en tanto captura de los datos objetivos que nos interesan para nuestro análisis.\n",
    "\n",
    "Una explicación breve de este código consistiría en señalar cómo el bot irá obteniendo los hrefs que contienen las url objetivo y una vez completada esa página, hace click en el botón siguiente para repetir el proceso con la siguiente página. Este proceso (el bucle) se dentendrá cuando la url anterior coincida con la recien url visitada tras pulsar el botón de siguiente: no hay más páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab03079-49f9-4a9c-bf45-f435a9f0118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def parse_url(url):\n",
    "   \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "   \n",
    "    final = []\n",
    "    next_bool  = True\n",
    "    while next_bool:\n",
    "        elem = driver.find_elements(By.CLASS_NAME, \"result-title\")\n",
    "       \n",
    "        for e in elem:\n",
    "            p = e.find_element(By.TAG_NAME, \"a\")\n",
    "            final.append(p.get_attribute(\"href\"))\n",
    "            print(\"Resultado final - conjunto de URLs objetivo -\"\n",
    "            print(final)\n",
    "       \n",
    "        try:\n",
    "            url_after = driver.current_url\n",
    "            next_  = driver.find_element(By.CLASS_NAME,\"rec-navigation\")\n",
    "            next_1 = next_.find_elements(By.TAG_NAME ,\"a\")\n",
    "            next_1[len(next_1)-1].click()\n",
    "\n",
    "            url_before = driver.current_url\n",
    "            if url_after == url_before:\n",
    "                next_bool = False\n",
    "            else:\n",
    "                next_bool = True\n",
    "\n",
    "        except:\n",
    "            next_bool = False\n",
    "        \n",
    "    driver.close()\n",
    "    return final\n",
    " \n",
    "url = \"https://digitallibrary.un.org/search?ln=en&cc=Voting+Data&p=&f=&action_search=Search&rm=&sf=&so=d&rg=50&c=Voting+Data&c=&of=hb&fti=0&fti=0\"\n",
    "\n",
    "\n",
    "parsed_tree = parse_url(url)\n",
    "\n",
    "\n",
    "if parsed_tree is not None:\n",
    "    file = open(\"hrefs\",\"w\",encoding=\"utf-8\")\n",
    "    file.write(str(parsed_tree))\n",
    "    file.close()\n",
    "    print(\"File printed\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152432d-4144-4412-b28a-567e5b5139fe",
   "metadata": {},
   "source": [
    "La celda inmediatamente inferior está pensada para ahorrar costes computacionales y de tiempo de cara a una presentación en video de este proceso, limitando el número de url extraidas que va a visitar el siguiente script para la captura y guardado de la porción de html donde se encuentra la información que nos interesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593309e-3f93-4f30-a499-3ec7a66196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_ = 7 # Cantidad de urls objetivo de las que vamos a extraer información con fines de abreviar un proceso muy largo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0326a-2818-4d75-ab4d-ef54ddf1adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "El script que se muestra en la celda siguiente tiene la función ya comentada: visitar cada url anteriormente extraída y guardar en disco (./dowloads) una porción de html donde se encuentra la información que nos interesa\n",
    "\n",
    "Para ello, usa el webdriver de firefox por cada url y selecciona una porción de html que guardará en disco mientras informa de cómo está resultado todo el procedemiento mediante la salida por consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786d69d-3ef1-45c5-868d-28175a2edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def parse_url(url):\n",
    "\n",
    "    os.makedirs(\"dowloads\", exist_ok=True)\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    result = driver.find_element(By.ID,\"details-collapse\")\n",
    "    result = result.get_attribute(\"innerHTML\")\n",
    "    result = result + \"\"\n",
    "   \n",
    "    driver.close()\n",
    "    return result\n",
    "   \n",
    "        \n",
    "# def start():\n",
    "\n",
    "f   = open(\"hrefs\", mode=\"r\", encoding=\"utf-8\")\n",
    "url = f.read()\n",
    "url = ast.literal_eval(url)\n",
    "numurl = len(url)\n",
    "print(\"Cantidad de urls: \"+str(numurl))\n",
    "\n",
    "count = 0\n",
    "for u in url:\n",
    "    if break_ == count:\n",
    "        print(\"fin del procedimiento abreviado\")\n",
    "        break\n",
    "    count += 1\n",
    "    parsed_tree = parse_url(u)\n",
    "    \n",
    "    if parsed_tree is not None:\n",
    "        i = url.index(u)\n",
    "        file = open(\"dowloads/contents_\"+str(i),\"w\",encoding=\"utf-8\")\n",
    "        file.write(str(parsed_tree))\n",
    "        file.close()\n",
    "        if i%10==0:\n",
    "            print(\"File printed -- \"+str(i+1)+\" elementos de \"+str(numurl)+\" totales\")\n",
    "        else:\n",
    "            print(\"File printed\")\n",
    "print(\"The End\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05540e5-37cc-47cd-94e9-e68fb2303ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Una vez finalizado el proceso debemos disponer de las porciones de código extraídos en la carpeta dowloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d0b9e-6c14-40b7-aaa3-9b8007101fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A continuación, realizaremos un procesamiento de las porciones de html extraidas, imprimiento por consola todos los datos probablemente revelevantes para nuestro análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0d47b-a87c-4a07-ac6d-11a671064fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Usaremos la biblioteca lxml de python que nos permite operar con el DOM tanto de XML como de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e52c7-8b69-4467-8a82-e160b6518372",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced079a3-4e20-4510-b036-fe7859b57a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "\n",
    "# file = open(\"dowloads/contents_0\")\n",
    "arr = os.listdir('dowloads')\n",
    "for file in arr:\n",
    "    file = open(\"dowloads/\"+file)\n",
    "    txt  = file.read()\n",
    "    file.close()\n",
    "    parser = etree.HTMLParser()\n",
    "    t=etree.fromstring(txt,parser)\n",
    "    count = -1\n",
    "    divs  = []\n",
    "    for e in t.iter():\n",
    "        count = count + 1 \n",
    "        if e.tag == \"div\":\n",
    "            divs.append(e)\n",
    "\n",
    "    notVotes = []\n",
    "    print(len(divs))\n",
    "    count = -1\n",
    "    for e in divs:\n",
    "        count = count + 1\n",
    "        try:\n",
    "         match count:\n",
    "                case 0:\n",
    "                    print(\"---------------------------title\")\n",
    "                    print(e.getchildren()[1].text)\n",
    "                    print(count)   \n",
    "                case 1:\n",
    "                    print(\"---------------------------Agenda\")\n",
    "                    print(e.getchildren()[1].getchildren()[0].text) \n",
    "                    print(count)   \n",
    "                case 2:\n",
    "                    print(\"---------------------------resolution\")\n",
    "                    print(e.getchildren()[1].getchildren()[0].text) \n",
    "                    print(count)   \n",
    "                case 7:\n",
    "                    print(\"---------------------------Summary\")\n",
    "                    print(e.getchildren()[1].getchildren()[1].text) \n",
    "                    print(count)   \n",
    "                case 10:\n",
    "                    print(\"---------------------------DATE\")\n",
    "                    print(e.getchildren()[1].text) \n",
    "                case 11:\n",
    "                    try:\n",
    "                        print(\"---------------------------Vote\")\n",
    "                        data = html.tostring(e.getchildren()[1], method='text', encoding='unicode')\n",
    "                        print(data) \n",
    "                    except:\n",
    "                        notVotes.append(\"file\") # guardamos aquellas resoluciones que no tienen aún votos para su exclusión posterior   \n",
    "        except:\n",
    "            print(\"hubo algún campo vacío:\")\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e6477-f4b9-43d6-baf5-808274c6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tras este procedimiento, estamos en disposción de dar estructura a los datos según los requerimientos del análisis de datos, bien sea en JSON, CSV o cualquier otro conveniente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
